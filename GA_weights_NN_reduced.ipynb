{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f480e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 14:43:08.183355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 14:43:08.324883: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-03 14:43:08.327816: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-03 14:43:08.327826: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-03 14:43:08.698028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-03 14:43:08.698078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-03 14:43:08.698080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/baccus/.local/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from deap import base,creator,tools\n",
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import sys\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow_addons as tfa \n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ae9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_pos = {\"A\":0,\"C\":1,\"D\":2,\"E\":3,\"F\":4,\"G\":5,\"H\":6,\"I\":7,\"K\":8,\"L\":9,\"M\":10,\"N\":11,\"P\":12,\"Q\":13,\"R\":14,\"S\":15,\"T\":16,\"V\":17,\"W\":18,\"Y\":19,\"-\":20}\n",
    "\n",
    "def vectorize_peptide(peptide_list): #sparse encoding\n",
    "    pept_vector = np.zeros((len(peptide_list),100,21,1))\n",
    "    counter = 0\n",
    "    counter_list = 0\n",
    "    for peptide in peptide_list:\n",
    "        if type(peptide) == float:\n",
    "            continue\n",
    "        if len(peptide) > 100:\n",
    "            peptide = peptide[0:100]\n",
    "        for i in range(len(peptide)):\n",
    "            pept_vector[counter_list,i,aa_pos[peptide[i]]]=1\n",
    "            counter += 1\n",
    "        for j in range(counter,100):\n",
    "            pept_vector[counter_list,j,aa_pos[\"-\"]]=1\n",
    "        counter_list +=1\n",
    "        counter = 0\n",
    "\n",
    "    pept_vector = np.squeeze(pept_vector)\n",
    "    return pept_vector\n",
    "\n",
    "def threemers(peptide_list): #threemers encoding\n",
    "    pept_vector = np.zeros((len(peptide_list),8000))\n",
    "    counter_list = 0\n",
    "    for peptide in peptide_list:\n",
    "        counts = {aa_comb: 0 for aa_comb in combinations_list}\n",
    "        for i in range(len(peptide)-2):\n",
    "            window = peptide[i:i+3]\n",
    "            if window in counts:\n",
    "                counts[window] += 1\n",
    "        pept_vector[counter_list]=list(counts.values())\n",
    "        counter_list +=1\n",
    "    pept_vector = pept_vector.reshape(pept_vector.shape[0],100,80)\n",
    "    return pept_vector\n",
    "\n",
    "def mutate_medium(weights_vector): #mutation of the individual, 25% chance for each parameter\n",
    "    for i in range(len(weights_vector[0])):\n",
    "        if i < 2:\n",
    "            probability = np.random.randint(101)\n",
    "            if probability <= 25: # 25% of probability to mutate\n",
    "                weights_vector[0][i]=random.uniform(1,10)\n",
    "        elif 2 <= i < 5:\n",
    "            probability = np.random.randint(101)\n",
    "            if probability <= 25: # 25% of probability to mutate\n",
    "                weights_vector[0][i]=random.randint(5,200)\n",
    "        else:\n",
    "            probability = np.random.randint(101)\n",
    "            if probability <= 25: # 25% of probability to mutate\n",
    "                weights_vector[0][i]=random.uniform(0.0001,1)\n",
    "    return weights_vector\n",
    "\n",
    "\n",
    "def prules(): #rules to create the parameters\n",
    "    weights=[]\n",
    "    for i in range(2): #possible weights of the two classe\n",
    "        weights.append(random.uniform(0.1,10))\n",
    "    for i in range(3): #kernel size\n",
    "        weights.append(random.randint(5,500))\n",
    "    for i in range(6): #learning and deacy rates\n",
    "        weights.append(random.uniform(0.0001,0.1))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def evaluate_medium(weights_vector): #basically the neural network that \n",
    "    weights_vector = weights_vector[0]\n",
    "    save_dir = \"/home/baccus/Desktop/PRIN/models/\"\n",
    "    import keras #keras needs to be imported each time otherwise the models will not load properly\n",
    "    from tensorflow.keras import regularizers\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    from keras import regularizers\n",
    "    from keras import metrics\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "    randomic_num = random.randint(0,200000000) #da cambiare come vengono registrati i modelli\n",
    "    METRICS = [keras.metrics.CategoricalAccuracy(name='categorical_accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc',multi_label=True),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'),\n",
    "          tfa.metrics.MatthewsCorrelationCoefficient(num_classes = 2,name=\"MCC\")]\n",
    "    save_dir = \"/home/baccus/Desktop/PRIN/models/\"       \n",
    "    functions = peptides_df[\"Class\"]\n",
    "    peptides_train_60 = peptides_df.sample(frac = 0.6)\n",
    "    peptides_test_20 = peptides_df.drop(peptides_train_60.index)\n",
    "    peptides_validate_20 = peptides_test_20.sample(frac = 0.5)\n",
    "    peptides_test_20 = peptides_test_20.drop(peptides_validate_20.index)\n",
    "    \n",
    "    all_data = vectorize_peptide(peptides_df[\"Sequence\"])\n",
    "    train_data = vectorize_peptide(peptides_train_60[\"Sequence\"])\n",
    "    test_data = vectorize_peptide(peptides_test_20[\"Sequence\"])\n",
    "    validation_data = vectorize_peptide(peptides_validate_20[\"Sequence\"])\n",
    "    all_labels = tensorflow.keras.utils.to_categorical(peptides_df[\"Class\"].tolist(), num_classes=2)\n",
    "    a = tensorflow.keras.utils.to_categorical(peptides_train_60[\"Class\"].tolist(), num_classes=2)\n",
    "    b = tensorflow.keras.utils.to_categorical(peptides_test_20[\"Class\"].tolist(), num_classes=2)\n",
    "    c = tensorflow.keras.utils.to_categorical(peptides_validate_20[\"Class\"].tolist(), num_classes=2)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(save_dir+str(randomic_num)+\".hdf5\", monitor='val_MCC', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model = models.Sequential()\n",
    "    weights = {0:weights_vector[0], 1:weights_vector[1]}\n",
    "    print(test_data[0].shape)\n",
    "    model.add(layers.Conv1D(weights_vector[3], (4,), kernel_regularizer=regularizers.L2(l2=weights_vector[5]), \n",
    "        bias_regularizer=regularizers.L2(l2=weights_vector[6]),activation='relu', input_shape=train_data[0].shape))\n",
    "    model.add(layers.MaxPooling1D((2,), padding=\"valid\"))\n",
    "    model.add(layers.Conv1D(weights_vector[4], (4,), activation='relu',kernel_regularizer=regularizers.L2(l2=weights_vector[7]), bias_regularizer=regularizers.L2(l2=weights_vector[8])))\n",
    "    model.add(layers.MaxPooling1D((2,)))\n",
    "    model.add(layers.Dropout(weights_vector[9]))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(weights_vector[2], activation='relu'))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    opt = keras.optimizers.Adam(learning_rate=weights_vector[10])\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=METRICS)\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=METRICS)\n",
    "    model.summary()\n",
    "    history = model.fit(train_data, a,class_weight=weights, validation_data=(validation_data,c), callbacks=callbacks_list, epochs=50, batch_size=20,use_multiprocessing=False,workers=1,verbose=1)\n",
    "    his = model.evaluate(test_data,b)\n",
    "    return (his[-1],his,randomic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "602c40f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MAIN GENETIC ALGORITHM\n",
    "def main():\n",
    "    parallel_processes = 1 #number of parallel processes that run at the same time, it is equal to the number of individuals in the first generation\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,)) #fitness is positive so we maximize it(MCC)\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "    if __name__ == \"__main__\":\n",
    "        #multiprocessing\n",
    "        pool = multiprocessing.Pool(processes=parallel_processes)\n",
    "        #create deap tools\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"map\", pool.map)\n",
    "        toolbox.register(\"evaluate\", evaluate_medium)\n",
    "        toolbox.register(\"select\", tools.selBest, k = 5) #selection algorithm selects the indivudial based on the best fitness\n",
    "        toolbox.register(\"individual_creator\", prules) #individual creator based on the rules \n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.individual_creator, 1)\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) #population takes individual creator function and repeats it multiple times to create the population for the GA\n",
    "        #initialize population\n",
    "        individuals_list = []\n",
    "        generations = []\n",
    "        fitnesses_list = []\n",
    "        pop = toolbox.population(n=parallel_processes)\n",
    "        for g in range(20): #generations\n",
    "            start_time = time.time()\n",
    "            fitnesses=toolbox.map(toolbox.evaluate, pop) #test the indivduals by running one instance of the NN network\n",
    "            for ind, fit in zip(pop, fitnesses): # manually register the fitness due to issues with DEAP classes\n",
    "                ind.fitness.values = (fit[0],)\n",
    "                fitnesses_list.append(fit)\n",
    "                individuals_list.append(ind)\n",
    "                generations.append(g)\n",
    "            best_individuals = toolbox.select(pop) #selection of best individuals\n",
    "            print(\"BEST INDIVS\")\n",
    "            print(best_individuals)\n",
    "            offsprings = best_individuals[:]\n",
    "            for i in range(15): #generate offpring by cloning best individuals and mutating them; no crossover\n",
    "                offspring = random.choice(best_individuals)\n",
    "                offspring = toolbox.clone(offspring)\n",
    "                offspring = mutate_medium(offspring)\n",
    "                offsprings.append(offspring)\n",
    "            pop = offsprings[:]\n",
    "        pool.close()\n",
    "        return individuals_list,fitnesses_list,generations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c68528fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Classificazione di  Antidiabetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baccus/.local/lib/python3.10/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/baccus/.local/lib/python3.10/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "2023-08-03 14:45:42.082714: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-08-03 14:45:42.082738: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-03 14:45:42.082764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bioinfo): /proc/driver/nvidia/version does not exist\n",
      "2023-08-03 14:45:42.083068: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 21)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 97, 409)           34765     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 48, 409)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 45, 371)           607327    \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 22, 371)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 371)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8162)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 423)               3452949   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 848       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,095,889\n",
      "Trainable params: 4,095,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      " 81/120 [===================>..........] - ETA: 1s - loss: 750.4110 - categorical_accuracy: 0.7284 - precision: 0.7284 - recall: 0.7284 - auc: 0.4990 - prc: 0.7168 - MCC: -0.0241"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m peptides_df\u001b[38;5;241m.\u001b[39mloc[peptides_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m      9\u001b[0m functions \u001b[38;5;241m=\u001b[39m peptides_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m best_i, best_f, best_g \u001b[38;5;241m=\u001b[39m main()\n\u001b[1;32m     11\u001b[0m best_res \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerations\u001b[39m\u001b[38;5;124m\"\u001b[39m : best_g ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndivuduals\u001b[39m\u001b[38;5;124m\"\u001b[39m : best_i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitnesses\u001b[39m\u001b[38;5;124m\"\u001b[39m:best_f})\n\u001b[1;32m     12\u001b[0m best_res\u001b[38;5;241m.\u001b[39mto_csv(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [5], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m): \u001b[38;5;66;03m#generations\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 24\u001b[0m     fitnesses\u001b[38;5;241m=\u001b[39m\u001b[43mtoolbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#test thye indivduals by running one instance of the NN network\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pop)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(fitnesses)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/tmp/ipykernel_78173/493337925.py\", line 116, in evaluate_medium\n",
      "    history = model.fit(train_data, a,class_weight=weights, validation_data=(validation_data,c), callbacks=callbacks_list, epochs=50, batch_size=20,use_multiprocessing=False,workers=1,verbose=1)\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
      "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
      "    return concrete_function._call_flat(\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/baccus/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#Load dataset and launch the GA\n",
    "peptides_df = pd.read_csv(\"db/DB_hyperfiltered.csv\")\n",
    "funzioni = peptides_df.Function.unique()\n",
    "for i in funzioni:\n",
    "    peptides_df = pd.read_csv(\"db/DB_hyperfiltered.csv\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Classificazione di \",i)\n",
    "    peptides_df.loc[peptides_df[\"Function\"] == i, \"Class\"] = 0 \n",
    "    functions = peptides_df[\"Class\"]\n",
    "    best_i, best_f, best_g = main()\n",
    "    best_res = pd.DataFrame({\"Generations\" : best_g ,\"Indivuduals\" : best_i, \"Fitnesses\":best_f})\n",
    "    best_res.to_csv(i+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c60a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
